\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[L]{\today}
\fancyfoot[C]{Notes on DeePC and DRO}
\fancyfoot[RO]{\thepage}

\title{DeePC and DRO Notes}
\author{Alessandro Quattrociocchi}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section{Problem Formulation}
Consider a deterministic LTI system whose model is unknown. 
\noindent Let $T_f \in \mathbb{Z}_{> 0}$ be the prediction horizon. Let $f_1 : \mathbb{R}^{mT_f} \rightarrow \mathbb{R}_{\geq0}$, and $f_2 : \mathbb{R}^{pT_f} \rightarrow \mathbb{R}_{\geq0}$ be a cost function on the future inputs and, respectively, outputs. We aim at finding a input sequece $\text{col}(u_t, \dots, u_{t+T_f-1}) \in \mathbb{R}^{mT_f}$, such that $\text{col}(y_t, \dots, y_{t+T_f-1}) \in \mathbb{R}^{pT_f}$ minimizes the cost $f_1 + f_2$, and the contraints satisfied, i.e., $u \in \mathcal{U}$ and $y \in \mathcal{Y}$, with $\mathcal{U} \subseteq \mathcal{R}^{mT_f}$ and $\mathcal{Y} \subseteq \mathcal{R}^{pT_f}$.

\subsection{Conventional Formulation}
Using conventional input output state space representation. 
\begin{align} \label{eq:conventional}
    \min_{u,y,x} \quad & f_1(u) +f_2(y) \notag\\
    \text{s.t.} \quad & \forall k \in {\{0, \dots, T_f -1\}}\notag\\
    \quad & {x}_{k+1} = Ax_k + Bu_k \notag\\
    \quad & {y}_{k} = Cx_k + Du_k \notag\\
    & x_0 = \hat{x}_t \notag\\
    & u \in \mathcal{U}, \: y \in \mathcal{Y} 
\end{align}

\subsection{Deterministic DeePC Formulation}
From data we contruct $\hat{U}_p, \hat{U}_f, \hat{Y}_p$, $\hat{Y}_f$.
\begin{align}
    \min_{g} \quad & f_1(u) + f_2(y) \notag\\[0.2cm]
    \text{s.t.} 
    &\begin{bmatrix} 
        \hat{U}_p \\ 
        \hat{Y}_p \\ 
        \hat{U}_f \\ 
        \hat{Y}_f 
        \end{bmatrix}g = \begin{bmatrix} 
            u_{\text{ini}} \\ 
            y_{\text{ini}}\\ 
            u \notag \\ 
            y
            \end{bmatrix}\\[0.2cm]
            & u \in \mathcal{U}, \: y \in \mathcal{Y} 
\end{align}

\begin{itemize}
    \item \( U_p, Y_p \) represent the past input-output trajectories.
    \item \( U_f, Y_f \) represent the future input-output trajectories.
\end{itemize}

\subsubsection{Remark I: \textit{DeepC representation of predictor.}}

\begin{align}
    \begin{bmatrix}
        u_{\text{ini}} \\
        y_{\text{ini}} \\
        u \\
        y
        \end{bmatrix}
        = H(w) g,  \quad \text{with} \quad 
            H(w) =
            \begin{bmatrix}
            U_p \\
            Y_p \\
            U_f \\
            Y_f
            \end{bmatrix}
\end{align}

where \( H(w) \) is the suitably partitioned Hankel matrix, or 
\begin{align}
    y = Y_f g
\end{align}
where \( g \) is computed from:
\[
\begin{bmatrix}
U_p \\
Y_p \\
U_f
\end{bmatrix} g = 
\begin{bmatrix}
u_{\text{ini}} \\
y_{\text{ini}} \\
u
\end{bmatrix}.
\]

\noindent The explicit solution for \( y \) is the superposition of a particolar solition and the homogenous term:

\[
y = Y_f \cdot \left(
\begin{bmatrix} U_p \\ Y_p \\ U_f \end{bmatrix}^+
\begin{bmatrix}
u_{\text{ini}} \\
y_{\text{ini}} \\
u
\end{bmatrix} + \text{ker} \left( \begin{bmatrix} U_p \\ Y_p \\ U_f \end{bmatrix} \right) \right),
\]
where \( \begin{bmatrix} U_p \\ Y_p \\ U_f \end{bmatrix}^+  \) is the pseudoinverse of the Hankel matrix, and \( \text{ker} \) represents the kernel of the matrix.


\subsubsection{Remark: \textit{Regularization in DeePC(rank-deficient Hankel matrices)}}

When the Hankel matrix \( H \) is rank-deficient (i.e., \( H \) has more columns than rows), there are infinitely many solutions \( g \) that satisfy the system equation:
\begin{align}
    H g &= 
    \begin{bmatrix}
        u_{\text{ini}} \\
        y_{\text{ini}} \\
        u
    \end{bmatrix}.
\end{align}

\noindent To find a noise-robust prediction, the Projection Regularizer is introduced. The regularizer penalizes the portion of \( g \) that lies in the nullspace of \( H \). This is achieved by minimizing:
\begin{align}
    \| (I - \Pi) g \|_2^2,
\end{align}
where:
\begin{itemize}
    \item \( \Pi \) is the orthogonal projection matrix onto the row space of \( H \),
    \item \( (I - \Pi) g \) isolates the component of \( g \) in \( \text{ker}(H) \), the nullspace of \( H \).
\end{itemize}

This ensures that the computed \( g \) is the smallest norm solution to:
\begin{align}
    y &= Y_f g.
\end{align}

In the noiseless case (\( \lambda_s = 0 \)) with regularization constant \( \lambda_p \geq 0 \), the optimization problem ensures that both \( u \) and \( y \) match the reference trajectory \( u_r, y_r \), making it beneficial to use the regularizer with a sufficiently large \( \lambda_p \).

\section{Stochastic DeePC Formulation}
Following the formulation in \cite{huang2021decentralized}, we now extend the DeePC definition of the unknow LTI system by considering a disturbance vector $w_t \in \mathbb{R}^q$, such as: 

\begin{align}
    \begin{cases}
    x_{t+1} = Ax_t + Bu_t + Ew_t \\
    y_t = Cx_t + Du_t + Fw_t
    \end{cases}
\end{align}

where $E \in \mathbb{R}^{n \times q}$ and $F \in \mathbb{R}^{p \times q}$.
Therefore, the system is subject to an \textbf{unknow} and \textbf{uncontrollable} disturbance, whose past trajectory can be measured but whose future evolutions are unknown. Similar to $u^d$ and $y^d$, we can build the Hankel matrix of the disturbance $\mathcal{H}_{T_{ini + N}}$, splitted as follows: 

\begin{equation}
    \begin{bmatrix}
    W_\text{P} \\
    W_\text{F}
    \end{bmatrix} := \mathcal{H}_{T_\text{ini}+N}(w^\text{d})
    \tag{14}
    \end{equation}

\noindent with  $W_\text{P} \in \mathbb{R}^{qT_\text{ini} \times H_c}$ and $W_\text{F} \in \mathbb{R}^{qN \times H_c}$. Therefore, we assume exist $g \in \mathbb{R}^{H_c}$, so that: 

\begin{align}
        \begin{bmatrix}
        U_\text{P} \\
        W_\text{P} \\
        Y_\text{P} \\
        U_\text{F} \\
        W_\text{F} \\
        Y_\text{F}
        \end{bmatrix} g =
        \begin{bmatrix}
        u_\text{ini} \\
        w_\text{ini} \\
        y_\text{ini} \\
        u \\
        w \\
        y
        \end{bmatrix}
\end{align}

\noindent Unlike \cite{huang2021decentralized}, we DO NOT assume $w_{t} \in [\underline{w}
, \overline{w}]^q$. Why this? We assume that in optimal scheduling problems, a time series forecasting model is always subject to an error which in very few toy examples converges to white noise. Therefore, we assume the general case where the error term distribution $w_t$ is not known a priori. 
Therefore, we assume $\hat{W}_\text{P}$ and $\hat{W}_\text{F}$ realizations of random variables $W_\text{P}$ and $W_\text{F}$. Reformulating from \cite{coulson2021distributionally}, we define $\xi = (\xi_{1}^T, \dots, \xi_{p, (T_{\text{ini}}+T_{\text{f}})})$, with $\xi_i$ denoting the $i$th rows $(W_{\text{p}}^T, W_{\text{f}}^T)$. 

\textbf{Objective Function}

The objective function formulated in Eq.\ref{eq:conventional}, is therefore reformulated into its robust conterpart, namely: 

\begin{align}
   \min_{g} \sup_{\mathbb{Q} \in \hat{\mathcal{P}}}& \mathbb{E}_\mathbb{Q} \left[f_1(\hat{U}_\text{f}g) + f_2(Y_\text{f}g))\right] \notag \\[0.5cm]
   \text{s.t.} &\begin{bmatrix}
    U_\text{P} \\
    W_\text{P} \\
    Y_\text{P} \\
    U_\text{F} \\
    W_\text{F} \\
    Y_\text{F}
    \end{bmatrix} g =
    \begin{bmatrix}
    u_\text{ini} \\
    w_\text{ini} \\
    y_\text{ini} \\
    u \\
    w \\
    y
    \end{bmatrix} \notag \\[0.5cm]
   \sup_{\mathbb{Q} \: \in \hat{\mathcal{P}}}& \text{CVaR}_{1-\alpha}^{\mathbb{Q}}(h(y)) \leq 0.
\end{align}


\subsection{Wasserstein Distance}
Definition copied from \cite{coulson2021distributionally},

We now introduce the \textit{Wasserstein ambiguity set} centred around the so-called \textit{empirical distribution}, i.e., $\mathbb{\hat{P}} = \frac{1}{N}\sum_{i=1}^{N}\delta_{\xi}$ \cite{mohajerin2018data} where $\delta_{\xi}$ is the kroneker delta, poiting a discrete sampling from the true unknow distribution. 

\noindent  $\mathcal{M}(\Xi)$ being the set of all distributions $\mathbb{Q}$ supported on $\Xi$, such that $\mathbb{E}_\mathbb{Q}[\|\xi\|_r] < \infty$, where $\|\cdot\|_r$ is the $r$-norm for some $r \in [1,\infty]$.


The \textit{Wasserstein metric} $\text{d}_\text{W}: \mathcal{M}(\Xi) \times \mathcal{M}(\Xi) \to \mathbb{R}_{\geq 0}$ is defined as

\begin{align}
    \text{d}_\text{W}(\mathbb{Q}_1, \mathbb{Q}_2) := \inf_{\Pi} \left\{\int_{\Xi^2} \|\xi_1 - \xi_2\|_r \Pi(d\xi_1, d\xi_2)\right\}    
\end{align}


\noindent where $\Pi$ is a joint distribution of $\xi_1$ and $\xi_2$ with marginal distributions $\mathbb{Q}_1 \in \mathcal{M}(\Xi)$ and $\mathbb{Q}_2 \in \mathcal{M}(\Xi)$, respectively.
The Wasserstein metric can be viewed as a distance between probability distributions, where the distance is calculated via an optimal mass transport plan $\Pi$.

\subsection{Objective Function Reformulation}
By using the distributionally robust linear regression theorem [Theorem 4, II] \cite{shafieezadeh2019regularization}, the objective function can be refomulated as follows: 
\begin{align}
    \min_{g} \sup_{\mathbb{Q} \in \hat{\mathcal{P}}}& \mathbb{E}_\mathbb{Q} \left[f_1(\hat{U}_\text{f}g) + f_2(Y_\text{f}g))\right] = \min_{g, \tau, s_i} f_1(\hat{U}_{\text{f}}\;g) + \frac{1}{N}\sum_{i=1}^{N}f_2(\hat{Y}_{\text{p}}^{(i)}g) + L_{\text{obj}}\epsilon ||g||_{\text{r,*}}\notag \\[0.5cm] 
\end{align}


\subsection{Constraint Reformulation}
\begin{align}
    \sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} & \text{CVaR}_{1-\alpha}^{\mathbb{Q}}(h(W_\text{f}\;g)) \leq 0 \rightarrow\sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \inf_{\tau \in \mathbb{R}} -\tau\alpha + \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] \leq 0 \\[0.5cm]
    &-\tau\alpha + L_\text{con}\epsilon\|g\|_{r,*} + \frac{1}{N}\sum_{i=1}^N s_i \leq 0 \\
    &\tau + h(\hat{W}_\text{f}^{(i)}g) \leq s_i \quad \forall i \leq N \\
    &s_i \geq 0 \quad \forall i \leq N
\end{align}



\subsection{DRO Chance Constrained DeePC- Tractable Reformulation}

\begin{align}
    \min_{g,\tau,s_i} & f_1(\hat{U}_\text{f}\;g)  + \frac{1}{N}\sum_{i=1}^N \left(f_2(\hat{Y}_\text{f}^{(i)}g)\right) + L_\text{obj}\epsilon\|g\|_{r,*} \\
     \text{s.t.} \quad & \hat{U}_\text{P}g = \hat{u}_\text{ini} \\
     & \hat{Y}_\text{P}g = \hat{y}_\text{ini} \\
     & \hat{U}_\text{f}g \in \mathcal{U} \\
     & \hat{Y}_\text{f}g \in \mathcal{Y} \\
     & -\tau\alpha + L_\text{con}\epsilon\|g\|_{r,*} + \frac{1}{N}\sum_{i=1}^N s_i \leq 0 \\
     & \tau + h(\hat{W}_\text{f}^{(i)}g) \leq s_i \quad \\
     & \quad s_i \geq 0 \quad \forall i \leq N
\end{align}


\newpage

\section*{Appendix}

By definition of CVaR

\begin{align}
    \text{CVaR}_{1-\alpha}^{\mathbb{P}}(h(y)) \leq 0 \Longleftrightarrow \inf_{\tau \in \mathbb{R}} -\tau\alpha + \mathbb{E}_\mathbb{P}[(h(y) + \tau)_+] \leq 0.
\end{align}

This can be shown by multiplying the definition of CVaR by $\alpha > 0$ and changing $\tau$ to $-\tau$. Letting $\ell(g,\xi) := h(\xi_{p(T_\text{ini}+1)}^{\top}g,\ldots,\xi_{p(T_\text{ini}+T_f)}^{\top}g)$, the constraint $g \in G_\text{CVaR}$ is equivalent to $g$ satisfying

\begin{equation}
\sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \inf_{\tau \in \mathbb{R}} -\tau\alpha + \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] \leq 0
\end{equation}

We have

\begin{equation}
\sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \inf_{\tau \in \mathbb{R}} -\tau\alpha + \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] \leq \inf_{\tau \in \mathbb{R}} -\tau\alpha + \sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] 
\end{equation}


by the max-min inequality. From Lemma A.1 we have

\begin{align}
    \sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+]
\end{align}

\begin{align}
    = \inf_{\lambda \geq 0} \lambda\epsilon + \frac{1}{N}\sum_{i=1}^N \sup_{\xi \in \Xi}((\ell(g,\xi) + \tau)_+ - \lambda\|\xi - \hat{\xi}^{(i)}\|_r)
\end{align}

Recalling that $\ell(g,\xi) = h(\xi_{p(T_\text{ini}+1)}^{\top}g,\ldots,\xi_{p(T_\text{ini}+T_f)}^{\top}g)$ and using Lemma A.2 gives

\begin{align}
    \sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] \leq \begin{cases} 
        \inf_{\lambda \geq 0} \lambda\epsilon + \frac{1}{N}\sum_{i=1}^N (\ell(g,\hat{\xi}^{(i)}) + \tau)_+ & \text{if } L_\text{con}\|g\|_{r,*} \leq \lambda \\
        \infty, & \text{otherwise.}
        \end{cases}
\end{align}

Hence, by resolving the inf and resorting to an epigraph formulation, we arrive at

\begin{align}
    &\inf_{\tau \in \mathbb{R}} -\tau\alpha + \sup_{\mathbb{Q} \in B_\epsilon(\hat{\mathbb{P}}_N)} \mathbb{E}_\mathbb{Q}[(\ell(g,\xi) + \tau)_+] \\
    &\leq \begin{cases} 
        \inf_{\tau \in \mathbb{R}, s_i \in \mathbb{R}} -\tau\alpha + \epsilon L_\text{con}\|g\|_{r,*} + \frac{1}{N}\sum_{i=1}^N s_i \\
        \text{s.t. } \tau + \ell(g,\hat{\xi}^{(i)}) \leq s_i \\
        s_i \geq 0 \\
        \forall i \leq N
        \end{cases}
\end{align}

Substituting the definition for $\ell$ and recalling from (13) that we wish for the right hand side of the above inequality to be nonnegative gives

\begin{align}
    \begin{cases}
        \inf_{\tau,s_i} -\tau\alpha + L_\text{con}\epsilon\|g\|_{r,*} + \frac{1}{N}\sum_{i=1}^N s_i \leq 0 \\
        g \mid \tau + h(\hat{Y}_\text{f}^{(i)}g) \leq s_i \\
        s_i \geq 0 \\
        \forall i \leq N
        \end{cases} \subseteq G_\text{CVaR}.
\end{align}


\bibliographystyle{plain} 
\bibliography{references.bib}

\end{document}